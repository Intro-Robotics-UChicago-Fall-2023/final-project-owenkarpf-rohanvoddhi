import math
import os
import subprocess
import time
from os import path
import os
import sys
from math import atan2

import numpy as np
import rospy
from gazebo_msgs.msg import ModelState
from geometry_msgs.msg import Twist, Point, Pose, Vector3, Quaternion
from gazebo_msgs.srv import SetModelState
from sensor_msgs.msg import LaserScan, Image
from nav_msgs.msg import Odometry
from std_srvs.srv import Empty
import cv2
import cv_bridge

from tf.transformations import quaternion_from_euler, euler_from_quaternion

from models import ActorNetwork
import torch

# SET PATH SO IT WORKS
PATH = ...
MAX_ANGULAR = 1.1
STATE_DIM = 36
ACTION_DIM = 2

class RunModelRealWorld():
    def __init__(self):
        self.bridge = cv_bridge.CvBridge()
        self.vel = Twist()
        self.velocity_publisher = rospy.Publisher('/cmd_vel', Twist, queue_size=10)
        self.vel.linear = Vector3(0, 0, 0)
        self.vel.angular = Vector3(0, 0, 0)
        self.scan_sub = rospy.Subscriber('/scan', LaserScan, self.store_scan_vals)
        self.image_sub = rospy.Subscriber('/camera/rgb/image_raw',
                                    Image, self.image_callback)
        self.actor_network = ActorNetwork(STATE_DIM, ACTION_DIM)
        self.actor_network.load_state_dict(torch.load(PATH + "/actor"))
        rospy.init_node("RealWorld")


    def run_model(self):
        while True:
            current_state = self.generate_state_from_scan()
            action = self.actor_network(current_state)
            scaled_action = self.scale_action(action)
            new_linear_vel, new_angular_vel = scaled_action
            self.vel.linear.x = new_linear_vel
            self.vel.angular.z = new_angular_vel
            self.velocity_publisher.publish(self.vel)
            time.sleep(.01)
            if self.is_at_target()[0] or self.is_at_wall():
                return

    def scale_action(self, action):
        new_action = np.zeros(2)
        new_action[0] = (action[0] + 1) * .13
        new_action[1] = action[1] * MAX_ANGULAR
        return new_action

    def store_scan_vals(self, scan_data):
        # Callback function for the lidar that stores lidar ranges
        self.ranges = list(scan_data.ranges)
        for i in range(len(self.ranges)):
            if self.ranges[i] == float('inf'):
                self.ranges[i] = 0

    def image_callback(self, msg):
        # Callback function that sets the image field to an rgb image generated by the camera
        image = self.bridge.imgmsg_to_cv2(msg,desired_encoding='bgr8')
        self.rgb_img = image

    def is_at_wall(self):
        # check if the robot is at the wall based on if any of the lidar scans are less than a threshold (.17)
        for dist in self.ranges:
            if dist == float('inf') or dist <= .17:
                return True
        return False

    def is_at_target(self):
        # Determine if the robot has reached the target by looking at the image recieved by the robot
        image_x = self.rgb_img.shape[1]
        lower_blue = np.array([80, 40, 20])
        upper_blue = np.array([100, 255, 255])
        hsv = cv2.cvtColor(self.rgb_img, cv2.COLOR_BGR2HSV)
        blue_mask = cv2.inRange(hsv, lower_blue, upper_blue)

        M_blue = cv2.moments(blue_mask)

        if M_blue['m00'] == 0:
            self.blue_x = 0
            self.blue_y = 0
            return (False, 200)

        elif M_blue['m00'] != 0:
            self.blue_x = int(M_blue['m10']/M_blue['m00'])
            self.blue_y = int(M_blue['m01']/M_blue['m00'])
        goal_x = image_x / 2
        dx = np.fabs(goal_x - self.blue_x)


        if dx < 50 and self.ranges[0] < .6:
            return (True, dx)
        return (False, dx)


    def generate_state_from_scan(self):
        # Codense LIDAR into smaller subsample to reduce training time
        # return self.ranges[:]
        condensed_states = []
        for i in range(36):
            current_subsample = self.ranges[i * 10: i * 10 + 10]
            condensed_states.append(np.min(current_subsample))
        return condensed_states
